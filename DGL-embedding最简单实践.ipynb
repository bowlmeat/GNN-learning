{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、Embedding的Hello World版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from dgl.nn import GraphConv \n",
    "from torch.optim import Adam\n",
    "\n",
    "num_nodes = 5\n",
    "emd_size = 5\n",
    "\n",
    "g = dgl.rand_graph(num_nodes=num_nodes, num_edges=25)\n",
    "embed = nn.Embedding(num_nodes, emd_size)\n",
    "\n",
    "# 有点迷不知道为什么这里不添加自环；在示例里是有 g = dgl.add_self_loop(g)\n",
    "# 没有自环时可添加：allow_zero_in_degree=True\n",
    "model = GraphConv(num_nodes, 1)\n",
    "\n",
    "# 需要注意你看，这里优化器的内容是包括这俩方面参数的\n",
    "optimizer = Adam(list(model.parameters()) + list(embed.parameters()), lr=1e-3) ### 这一句还漏写了\n",
    "\n",
    "labels = torch.zeros((num_nodes, 1))\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pred = model(g, embed.weight)\n",
    "    loss = criteria(pred, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "## 提醒：注意设置随机种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、加载数据\n",
    "预计需要结合：\n",
    "- mini-batch 小批量\n",
    "- 创建dataset与dataloader\n",
    "- 选择性读文件\n",
    "- 异构图    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.基本构建异构图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'game': 5, 'topic': 3, 'user': 2},\n",
      "      num_edges={('user', 'follows', 'topic'): 4, ('user', 'follows', 'user'): 4, ('user', 'plays', 'game'): 4},\n",
      "      metagraph=[('user', 'topic', 'follows'), ('user', 'user', 'follows'), ('user', 'game', 'plays')])\n",
      "---------------\n",
      "tensor([-0.3391,  0.4894, -1.3954, -0.5781])\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "data_dict = {\n",
    "    ('user','follows','user'):(torch.tensor([0, 0, 0, 0]), torch.tensor([0, 0, 0, 0])),\n",
    "    ('user','follows','topic'):(torch.tensor([0, 0, 0, 0]), torch.tensor([1, 2, 1, 1])),\n",
    "    ('user','plays','game'):(torch.tensor([0, 0, 0, 0]), torch.tensor([3, 4, 1, 1]))\n",
    "}\n",
    "num_nodes_dict = {'user': 2, 'topic': 3, 'game': 5}  # 通过这条语句修改一种类别的不同属性；注意这里的user设定！\n",
    "\n",
    "g = dgl.heterograph(data_dict, num_nodes_dict=num_nodes_dict)\n",
    "\n",
    "g.nodes['user'].data['x'] = torch.randn(2, 4)  # (5, 4)->(1, 4)错误原因在于只有一个user节点；看ID\n",
    "\n",
    "print(g)\n",
    "print('---------------')\n",
    "print(g.nodes['user'].data['x'][1])\n",
    "\n",
    "# g.data['x'][i] will give the feature 'x' of the node with ID i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.尝试读取数据\n",
    "- 阅读他人代码得到的注意点：\n",
    "  - ***Batching test data for inference***:\n",
    "     1. `.numpy()`：转化\n",
    "     2. `torch.unique`：除去多余重复部分\n",
    "     3. \n",
    "       ```\n",
    "       sampler = dgl.dataloading.MultiLayerNeighborSampler([15, 10, 5])\n",
    "       dataloader = dgl.dataloading.NodeDataLoader(g, ...)\n",
    "       # ------\n",
    "       sampler = dgl.dataloading.MultiLayerFullNeighborSampler(3)\n",
    "       ```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识点搜集：\n",
    " - features\n",
    "   - ***in_features:*** corresponds to the size of your input features.\n",
    "   - ***out_features:*** corresponds to the size of your output, usually the number of classes for classification or 1 for regression.\n",
    "   - ***hidden_features:*** corresponds to the size of your hidden state, where you set it as a hyperparameter.\n",
    " - You can set out_features to be the same as hidden_features so that the representations can be fed into the score predictor to produce the final positive/negative scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PS:\n",
    "1.inference层的作用，推断embedding\n",
    "  - inference: 外层循环layers，内层循环sampling等采样\n",
    "  - training: 相反(同时内层还有消息传递等)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- ---- minibatch training ------ ------ #\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoLayerGCN(nn.Module):\n",
    "    def _init_(self, in_features, hidden_features, out_features):\n",
    "        super()._init_()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_features, hidden_features)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_features, out_features)\n",
    "    \n",
    "    def forward(self, g, h):  # g->blocks\n",
    "        h = F.relu(self.conv1(g, h))  # g->blocks[0]\n",
    "        h = F.relu(self.conv2(g, h))  # g->blocks[1]\n",
    "        return h\n",
    "\n",
    "# heterogeneous 注意这其中的不同点：\n",
    "self.conv1 = dgl.nn.heteroGraphConv({rel: dgl.nn.GraphConv() for rel in rels})\n",
    "\n",
    "h = self.conv1(g, h)\n",
    "h = {k: F.relu(v) for k, v in h.items()} # 字典形式\n",
    "return h\n",
    "\n",
    "# 训练部分\n",
    "model.train()\n",
    "logits = model(g, h)['user']\n",
    "loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. API接口使用与参数使用\n",
    "   - apply_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "## 使用apply_edges方法记录\n",
    "g = dgl.heterograph({('user','plays','game'): ([0, 1, 1, 2], [0, 0, 2, 1])})\n",
    "g.edges[('user','plays','game')].data['h'] = torch.ones(4, 5)\n",
    "g.apply_edges(lambda edges: {'h': edges.data['h'] * 2})\n",
    "\n",
    "# print(g.edges[('user','plays','game')].data['h'])\n",
    "print(g.edata['h'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 'follows', 'game'), ('user', 'follows', 'user'), ('user', 'plays', 'game')]\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "g = dgl.heterograph({\n",
    "    ('user', 'follows', 'user'): (torch.tensor([0, 1]), torch.tensor([1, 2])),\n",
    "    ('user', 'follows', 'game'): (torch.tensor([0, 1, 2]), torch.tensor([1, 2, 3])),\n",
    "    ('user', 'plays', 'game'): (torch.tensor([1, 3]), torch.tensor([2, 3]))\n",
    "})\n",
    "print(g.canonical_etypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follows', 'follows', 'plays']\n"
     ]
    }
   ],
   "source": [
    "t = [('user', 'follows', 'game'),\n",
    "     ('user', 'follows', 'user'),\n",
    "     ('user', 'plays', 'game')]\n",
    "res = [etype for utype, etype, vtype in t]\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.建图 $\\rightarrow$ 注意这里输入卷积层的特征和嵌入区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Bool type is not supported by dlpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1daf7f745f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRedditDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRedditDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_loop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\reddit.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, self_loop, raw_dir, force_reload, verbose)\u001b[0m\n\u001b[0;32m    135\u001b[0m                                             \u001b[0mraw_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                                             \u001b[0mforce_reload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                                             verbose=verbose)\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\dgl_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, url, raw_dir, hash_key, force_reload, verbose)\u001b[0m\n\u001b[0;32m    286\u001b[0m                                                 \u001b[0mhash_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhash_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                                                 \u001b[0mforce_reload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                                                 verbose=verbose)\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\dgl_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, url, raw_dir, save_dir, hash_key, force_reload, verbose)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\dgl_dataset.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done saving data into cached files.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\reddit.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mgraph_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dgl_graph.bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0msave_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\graph_serialize.py\u001b[0m in \u001b[0;36msave_graphs\u001b[1;34m(filename, g_list, labels)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mg_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mg_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_sample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDGLHeteroGraph\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Doesn't support DGLHeteroGraph's derived class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0msave_heterographs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         raise DGLError(\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\heterograph_serialize.py\u001b[0m in \u001b[0;36msave_heterographs\u001b[1;34m(filename, g_list, labels)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mg_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDGLHeteroGraph\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Invalid DGLHeteroGraph in g_list argument\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mgdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mHeteroGraphData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0m_CAPI_SaveHeteroGraphData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_dict_to_ndarray_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\heterograph_serialize.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mg_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDGLHeteroGraph\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Invalid DGLHeteroGraph in g_list argument\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mgdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mHeteroGraphData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0m_CAPI_SaveHeteroGraphData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_dict_to_ndarray_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\heterograph_serialize.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0medata_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict_to_ndarray_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mntype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mntypes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mndata_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict_to_ndarray_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mntype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_CAPI_MakeHeteroGraphData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mntypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\data\\heterograph_serialize.py\u001b[0m in \u001b[0;36mtensor_dict_to_ndarray_dict\u001b[1;34m(tensor_dict)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mndarray_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mndarray_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzerocopy_to_dgl_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_strmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndarray_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py\u001b[0m in \u001b[0;36mzerocopy_to_dgl_ndarray\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mzerocopy_to_dgl_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dlpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dlpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mzerocopy_to_dgl_ndarray_for_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Bool type is not supported by dlpack"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "from dgl.data import RedditDataset\n",
    "\n",
    "data = RedditDataset(self_loop=True)\n",
    "g = data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 1 0 2 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "# 设置种子\n",
    "torch.manual_seed(0)\n",
    "# 设定参数\n",
    "num_nodes = 6\n",
    "num_edges = 15\n",
    "# 加载数据\n",
    "src = []\n",
    "dst = []\n",
    "\n",
    "with open('signal_1.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for line in reader:\n",
    "        src.append(int(line[0]))\n",
    "        dst.append(int(line[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 抄来的\n",
    "class ConceptNetDataset(DGLDataset):\n",
    "    def __init__(self, path, sep):\n",
    "        self.path = path\n",
    "        self.sep = sep\n",
    "        super().__init__(name='concept_net')\n",
    "\n",
    "    def process(self):\n",
    "        bidirections = [\"RelatedTo\", \"Synonym\", \"Antonym\", \"DistinctFrom\",\n",
    "                        \"LocatedNear\", \"SimilarTo\", \"EtymologicallyRelatedTo\"]\n",
    "        data = pd.read_csv(self.path, sep=self.sep)\n",
    "        # get all the entities\n",
    "        nodes = pd.concat([data[\"e1\"], data[\"e2\"]], axis=0).unique()\n",
    "        edges_type = data[\"rel\"].unique().tolist()\n",
    "        edges = {y: x for x, y in enumerate(edges_type)}\n",
    "        entities = {y: x for x, y in enumerate(nodes)}\n",
    "        # encode all entities\n",
    "        data[\"e1\"] = data[\"e1\"].apply(lambda x: entities[x])\n",
    "        data[\"e2\"] = data[\"e2\"].apply(lambda x: entities[x])\n",
    "\n",
    "        # encode all entities in the nodes list\n",
    "        def encode(x): return entities[x]\n",
    "        nodes = [encode(x) for x in nodes]\n",
    "        nodes = np.array(nodes)\n",
    "        # create node labels\n",
    "        node_labels = torch.from_numpy(nodes)\n",
    "\n",
    "        # edge_features = torch.from_numpy(data['score'].to_numpy())\n",
    "        node_type = \"_N\"  # '_N' can be replaced by an arbitrary name\n",
    "        data_dict = dict()\n",
    "        num_nodes_dict = {node_type: len(entities)}\n",
    "\n",
    "        # create backlinks to node with certain edge types\n",
    "        for bd in bidirections:\n",
    "            aux = data[data[\"rel\"] == bd].copy()\n",
    "            col_list = list(aux)\n",
    "            col_list[0], col_list[1] = col_list[1], col_list[0]\n",
    "            aux.columns = col_list\n",
    "            aux = aux[sorted(aux)]\n",
    "            data = pd.concat([data, aux], axis=0, ignore_index=True)\n",
    "\n",
    "        data.reset_index(drop=True)\n",
    "        for e_t in edges_type:\n",
    "            aux = data[data[\"rel\"] == e_t]\n",
    "            src = torch.from_numpy(aux['e1'].to_numpy())\n",
    "            dst = torch.from_numpy(aux['e2'].to_numpy())\n",
    "            data_dict[(node_type, e_t, node_type)] = (src, dst)\n",
    "\n",
    "        self.graph = dgl.heterograph(data_dict, num_nodes_dict)\n",
    "        for e_t in edges_type:\n",
    "            # add the weitght to each node\n",
    "            self.graph.edges[e_t].data[\"weight\"] = torch.from_numpy(\n",
    "                data[data[\"rel\"] == e_t]['score'].to_numpy())\n",
    "            # add the train mask\n",
    "            e_len = len(data[data['rel'] == e_t])\n",
    "            self.graph.edges[e_t].data['train_mask'] = torch.zeros(\n",
    "                e_len, dtype=torch.bool).bernoulli(0.6)\n",
    "        # add a feature to each node, the feature is the index of the word in the vocab\n",
    "        self.graph.nodes['_N'].data[\"feature\"] = node_labels\n",
    "        # add the train tamsk to the nodes\n",
    "        self.graph.nodes['_N'].data[\"train_mask\"] = torch.zeros(\n",
    "            len(entities), dtype=torch.bool).bernoulli(0.6)\n",
    "\n",
    "        # Train val splti for node classification\n",
    "        n_nodes = nodes.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
